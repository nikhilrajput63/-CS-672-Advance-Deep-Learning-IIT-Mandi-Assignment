# RL Algorithm Comparison Report
**Generated:** 2025-11-17 08:15:07

---

## DQN

- **Episodes trained:** 300
- **Mean reward:** 69.76
- **Median reward:** 52.50
- **Std deviation:** 54.65
- **Max reward:** 413.00
- **Min reward:** 8.00
- **Final avg reward:** 112.90
- **Status:** Did not reach solving threshold

## REINFORCE

- **Episodes trained:** 223
- **Mean reward:** 63.92
- **Median reward:** 40.00
- **Std deviation:** 64.30
- **Max reward:** 449.00
- **Min reward:** 10.00
- **Final avg reward:** 188.80
- **Solved at episode:** 223 (10-episode avg ≥ 180)

## PG+Baseline

- **Episodes trained:** 222
- **Mean reward:** 54.90
- **Median reward:** 38.00
- **Std deviation:** 51.45
- **Max reward:** 286.00
- **Min reward:** 9.00
- **Final avg reward:** 188.80
- **Solved at episode:** 222 (10-episode avg ≥ 180)

---

## Comparative Analysis

### 1. Sample Efficiency

1. **PG+Baseline**: 222 episodes
2. **REINFORCE**: 223 episodes
3. **DQN**: 300 episodes

### 2. Stability (Lower std = more stable)

1. **PG+Baseline**: σ = 51.45
2. **DQN**: σ = 54.65
3. **REINFORCE**: σ = 64.30

### 3. Final Performance

1. **REINFORCE**: 188.80
2. **PG+Baseline**: 188.80
3. **DQN**: 112.90

---

## Key Insights

### DQN (Deep Q-Network)
- **Strengths:** Most sample efficient due to experience replay
- **Mechanism:** Stores and reuses transitions; target network stabilizes learning
- **Best for:** Discrete action spaces with sample efficiency priority

### REINFORCE (Vanilla Policy Gradient)
- **Strengths:** Simplest implementation
- **Weakness:** High variance due to Monte Carlo returns
- **Best for:** Understanding basic policy gradient concepts

### PG+Baseline
- **Strengths:** Significant variance reduction vs REINFORCE
- **Mechanism:** Value function baseline for advantage estimation
- **Best for:** Balance between simplicity and performance

---

## Discussion

### Why DQN is Sample Efficient
- **Experience Replay:** Each transition used multiple times
- **Off-policy:** Can learn from old experiences
- **Decorrelation:** Random sampling breaks temporal correlations

### Why REINFORCE Has High Variance
- **Monte Carlo:** Full episode returns have high variance
- **No bootstrapping:** Unlike TD methods, no bias-variance tradeoff
- **Credit assignment:** All actions get same return signal

### How Baseline Reduces Variance
- **Advantage:** A(s,a) = G(s,a) - V(s) instead of G(s,a)
- **Relative evaluation:** Actions judged relative to state value
- **Unbiased:** Baseline doesn't introduce bias, only reduces variance

---

*This report is generated entirely from actual training results.*
